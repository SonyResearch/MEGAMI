
defaults:
  - dset: tencymastering_vocals_simulated_server5
  - network: Latent_DiT_M2L4_crossatt_catabspos
  - diff_params: edm_style_AFxRep_MERT_simulated_fxnorm
  - tester: evaluate_style_conditional_dry_vocals_simulated_fxnorm
  - logging: base_logging

model_dir: "/data4/eloi/experiments/1C_tencymastering_vocals"



dset:
  train:
    segment_length: 525312  # Replace with your desired value
  validation:
    segment_length: 525312  # If you want to change validation segment length too
    num_examples: 512

exp:
  exp_name: "1C_tencymastering_vocals" #name of the experiment

  trainer:
    _target_: "training.trainer_ddp_simulated.Trainer"
   
  #main options
  #related to optimization
  optimizer:
    _target_: "torch.optim.AdamW"
    lr: 1e-4 #            help='Learning rate',
    betas: [0.9, 0.999]
    eps: 1e-8 #for numerical stability, we may need to modify it if usinf fp16
    weight_decay: 0.01 #weight decay for the optimizer
  

  lr_rampup_it: 1000 

  #for lr scheduler (not noise schedule!!) TODO (I think)
  scheduler_step_size: 60000
  scheduler_gamma: 0.8

  batch_size: 16

  val_batch_size: 16

  # Performance-related.
  num_workers: 8

  # I/O-related. moved to logging
  seed: 1 # random seed

  resume: False
  resume_checkpoint: None

  #audio data related
  sample_rate: 44100
  stereo: True
  audio_len: 525312

  ema_rate: 0.9999  
  ema_rampup: 10000  #linear rampup to ema_rate   


  #gradient clipping
  use_grad_clip: True
  max_grad_norm: 1

  restore : False
  checkpoint_id: None
  align: False

  augmentations:
    list: ["polarity" ]
    polarity:
      prob: 0.5

  compile: True

#testing (demos)

hydra:
  job:
    config:
      # configuration for the ${hydra.job.override_dirname} runtime variable
      override_dirname:
        kv_sep: '='
        item_sep: ','
        # Remove all paths, as the / in them would mess up things
        exclude_keys: ['path_experiment',
          'hydra.job_logging.handles.file.filename']