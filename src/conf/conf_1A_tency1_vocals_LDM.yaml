
defaults:
  - dset: tency1_vocals
  - network: Latent_DiT_SAO
  - diff_params: edm_LDM
  - tester: unconditional_vocals
  - logging: base_logging

model_dir: "experiments/1A_tency1_vocals"


exp:
  exp_name: "1A_tency1_vocals" #name of the experiment

  model_dir: None #directory where the model will be saved loally

  trainer:
    _target_: "training.trainer.Trainer"
   
  #main options
  #related to optimization
  optimizer:
    _target_: "torch.optim.Adam"
    lr: 1e-4 #            help='Learning rate',
    betas: [0.9, 0.999]
    eps: 1e-8 #for numerical stability, we may need to modify it if usinf fp16
  

  lr_rampup_it: 10000 

  #for lr scheduler (not noise schedule!!) TODO (I think)
  scheduler_step_size: 60000
  scheduler_gamma: 0.8

  batch_size: 4 

  # Performance-related.
  num_workers: 4  

  # I/O-related. moved to logging
  seed: 1 # random seed

  resume: True
  resume_checkpoint: None

  #audio data related
  sample_rate: 44100
  audio_len: 262144

  ema_rate: 0.9999  
  ema_rampup: 10000  #linear rampup to ema_rate   


  #gradient clipping
  use_grad_clip: True
  max_grad_norm: 1

  restore : False
  checkpoint_id: None

  


#testing (demos)

hydra:
  job:
    config:
      # configuration for the ${hydra.job.override_dirname} runtime variable
      override_dirname:
        kv_sep: '='
        item_sep: ','
        # Remove all paths, as the / in them would mess up things
        exclude_keys: ['path_experiment',
          'hydra.job_logging.handles.file.filename']