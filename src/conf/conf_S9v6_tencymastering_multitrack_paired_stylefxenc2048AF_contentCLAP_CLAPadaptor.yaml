
defaults:
  - dset: fulltency_multitrack_server5
  - network: Latent_DiT_M2L4_crossatt_multitrack_notaxonomy
  - tester: evaluate_style_conditional_dry_multitrack_paired_Fxenc2048Fv6_fxnorm
  - logging: base_logging

model_dir: "/data5/eloi/experiments/1C_tencymastering_vocals"



dset:
  train:
    segment_length: 525312  # Replace with your desired value
  validation:
    segment_length: 525312  # If you want to change validation segment length too
    num_examples: 512

exp:
  exp_name: "1C_tencymastering_vocals" #name of the experiment

  trainer:
    _target_: "training.trainer_ddp_simulated.Trainer"
   
  #main options
  #related to optimization
  optimizer:
    _target_: "torch.optim.AdamW"
    lr: 1e-5 #            help='Learning rate',
    betas: [0.9, 0.999]
    eps: 1e-8 #for numerical stability, we may need to modify it if usinf fp16
    weight_decay: 0.01 #weight decay for the optimizer
  

  lr_rampup_it: 1000 

  #for lr scheduler (not noise schedule!!) TODO (I think)
  scheduler_step_size: 60000
  scheduler_gamma: 0.8

  batch_size: 8

  skip_first_val: False

  val_batch_size: 8

  # Performance-related.
  num_workers: 8

  # I/O-related. moved to logging
  seed: 1 # random seed

  resume: False
  resume_checkpoint: None

  #audio data related
  sample_rate: 44100
  stereo: True
  audio_len: 525312

  ema_rate: 0.9999  
  ema_rampup: 10000  #linear rampup to ema_rate   


  #gradient clipping
  use_grad_clip: True
  max_grad_norm: 1

  restore : False
  checkpoint_id: None
  align: False

  augmentations:
    list: ["polarity" ]
    polarity:
      prob: 0.5

  compile: True

  max_tracks: 14

diff_params:
  #Elucidated Diffusion Model (Karras et al. 2022)
  _target_: "diff_params.edm_style_multitrack.EDM_Style_Multitrack"

  # SDE hyperparameters
  type: "ve_karras"

  AE_type: "CLAP" #used for encoding the context

  FXenc_args:
    type: "fx_encoder2048+AFv6"

  sample_rate: 44100

  cfg_dropout_prob: 0.2

  sde_hp:
    sigma_data: 0.025 #default for maestro
    sigma_min: 5e-4
    sigma_max: 5
    max_sigma: 5
    rho: 10

  CLAP_args:
    ckpt_path: "/data5/eloi/checkpoints/laion_clap/music_audioset_epoch_15_esc_90.14.patched.pt"
    distance_type: "cosine"
    normalize: True #if True, the features will be normalized
    use_adaptor: True #if True, the features will be adapted to the CLAP space
    adaptor_checkpoint: "/data5/eloi/checkpoints/A2_CLAP_TM/MLP_CLAP_regressor-20000.pt"
    adaptor_type: "MLP_CLAP_regressor"
    add_noise: True #if True, the features will be augmented with orthogonal noise
    noise_sigma: 0.011 #sigma of the orthogonal noise to

  fx_encoder_plusplus_args:
    distance_type: "cosine"
    ckpt_path: "/home/eloi/projects/project_mfm_eloi/src/utils/feature_extractors/ckpt/fxenc_plusplus_default.pt"

  context_signal: "wet"

  apply_fxnormaug: False
  fxnormaug_train: Null

  fxnormaug_inference: Null

  default_shape: [1,3, 64, 33] 

network:
  _target_: "networks.dit_multitrack.DiffusionTransformer"

  io_channels: 64
  embed_dim: 512
  depth: 16
  num_heads: 16
  cond_token_dim: 64
  global_cond_dim: 0
  input_concat_dim: 0
  project_cond_tokens: false
  transformer_type: "continuous_transformer"

  pos_emb_strategy: "concatenation"
  pos_emb_dim: 64 # now dimension is 512+64       16 (+1) ideal for AFxRep  (size 512 x 2= 1024 = 64 x 16)
  pos_emb_type: "one-hot"

  pos_emb_crossattn_strategy: "concatenation"
  pos_emb_crossattn_dim: 64 # now dimension is 64+32  ideal for MERT (size 768 = 64 x 12)
  pos_emb_crossattn_type: "one-hot"

  use_taxonomy_in_pos_emb: False

#testing (demos)

hydra:
  job:
    config:
      # configuration for the ${hydra.job.override_dirname} runtime variable
      override_dirname:
        kv_sep: '='
        item_sep: ','
        # Remove all paths, as the / in them would mess up things
        exclude_keys: ['path_experiment',
          'hydra.job_logging.handles.file.filename']

